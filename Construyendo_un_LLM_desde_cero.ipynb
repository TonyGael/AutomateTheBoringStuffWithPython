{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7LybAvAqkAII7VUP3Vqm6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Link al libro que estoy usando para armar el LLM: [Build a Large Language Model from Scratch - Sebastian Raschka](https://drive.google.com/file/d/1WkucQZgK4RENhGG_lm75yRqcQWw-Pxzk/view?usp=sharing)\n"
      ],
      "metadata": {
        "id": "fXmyeoyi73kj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l55Q2tNWDdU",
        "outputId": "7d716207-ac5c-4b63-a623-abe8e117f1e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de caracteres: 20479.\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open('./samples/llm_from_scratch/the-verdict.txt', 'r', encoding='utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "print(f'Número total de caracteres: {len(raw_text)}.')\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hace esto, línea por línea:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "with open('./sample_data/llm_from_scratch/the-verdict.txt', 'r', encoding='utf-8') as file:\n",
        "```\n",
        "\n",
        "* `open(path, mode, encoding=...)`: abre un archivo y devuelve un objeto fichero.\n",
        "\n",
        "  * `path`: `'./sample_data/llm_from_scratch/the-verdict.txt'`. Ruta relativa al **directorio de trabajo actual**. En Colab suele ser `/content`. `./` apunta ahí.\n",
        "  * `mode='r'`: modo lectura de texto. No crea ni modifica el archivo.\n",
        "  * `encoding='utf-8'`: decodifica bytes a texto Unicode con UTF-8. Evita errores de acentos.\n",
        "* `with ... as file:` usa un **context manager**. Garantiza cierre del archivo al salir del bloque, incluso si hay excepciones. Evita fugas de descriptores.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "    raw_text = file.read()\n",
        "```\n",
        "\n",
        "* `file.read()` lee **todo** el contenido en memoria y retorna un `str` (Unicode). Complejidad O(n).\n",
        "* Útil para archivos pequeños/medianos. Para archivos muy grandes, preferir lectura por trozos o iterar líneas.\n",
        "\n",
        "3.\n",
        "\n",
        "```python\n",
        "print(f'Número total de caracteres: {len(raw_text)}.')\n",
        "```\n",
        "\n",
        "* `len(raw_text)`: número de **caracteres Unicode** del `str`. No son bytes.\n",
        "* Un “carácter” aquí es un **code point**. Algunos grafemas visibles pueden componerse de varios code points; `len` contaría más de uno en ese caso.\n",
        "* `f'...'`: f-string. Evalúa expresiones dentro de `{}` y las inserta en el texto.\n",
        "\n",
        "4.\n",
        "\n",
        "```python\n",
        "print(raw_text[:99])\n",
        "```\n",
        "\n",
        "* `raw_text[:99]`: **slice** desde el inicio hasta el índice 99 **excluido**. Devuelve como mucho 99 caracteres. Si el texto es más corto, no falla.\n",
        "* Útil para previsualizar el inicio del archivo.\n",
        "\n",
        "Notas prácticas en Colab:\n",
        "\n",
        "* Verifica que el archivo exista en esa ruta. Si falla, `FileNotFoundError`.\n",
        "* Si ves errores de codificación, confirma UTF-8 del archivo. Alternativa: `encoding='utf-8-sig'` si trae BOM, o `errors='replace'` para caracteres inválidos.\n",
        "* Ruta alternativa robusta:\n",
        "\n",
        "  ```python\n",
        "  from pathlib import Path\n",
        "  p = Path('sample_data/llm_from_scratch/the-verdict.txt')\n",
        "  with p.open('r', encoding='utf-8') as f:\n",
        "      raw_text = f.read()\n",
        "  ```\n",
        "* Si necesitas memoria estable, no uses `read()` con archivos muy grandes. Mejor:\n",
        "\n",
        "  ```python\n",
        "  size = 0\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "          size += len(line)\n",
        "  ```"
      ],
      "metadata": {
        "id": "aPJ47mqw2z4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = 'Hey, you. This, is a test.'\n",
        "result = re.split(r'\\s', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRAeN9-M5aPd",
        "outputId": "d274b1e6-e53f-4a31-e308-3d5fca072cba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey,', 'you.', 'This,', 'is', 'a', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación precisa línea por línea:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "import re\n",
        "```\n",
        "\n",
        "* Importa el módulo estándar **`re`** (regular expressions). Permite búsqueda, coincidencia y manipulación de texto mediante **expresiones regulares**.\n",
        "* Internamente compila patrones a autómatas finitos optimizados.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "text = 'Hey, you. This, is a test.'\n",
        "```\n",
        "\n",
        "* Declara una variable `text` tipo `str` con el contenido literal indicado.\n",
        "* Contiene palabras, comas, puntos y espacios.\n",
        "\n",
        "3.\n",
        "\n",
        "```python\n",
        "result = re.split(r'\\s', text)\n",
        "```\n",
        "\n",
        "* `re.split(pattern, string)` divide `string` en una lista, usando **las coincidencias del patrón** como separadores.\n",
        "* `r'\\s'` es un *raw string literal*: el prefijo `r` indica que `\\` se trata como carácter literal y no como escape de Python.\n",
        "* En una expresión regular, `\\s` coincide con **cualquier carácter de espacio en blanco**: espacio, tabulador, salto de línea, retorno de carro o tab vertical.\n",
        "* Por tanto, esta instrucción separa el texto cada vez que encuentra un espacio o cualquier carácter blanco.\n",
        "* Devuelve una lista de subcadenas sin los delimitadores (los espacios no se incluyen en la salida).\n",
        "\n",
        "4.\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* Imprime la lista resultante.\n",
        "* Salida esperada:\n",
        "\n",
        "  ```\n",
        "  ['Hey,', 'you.', 'This,', 'is', 'a', 'test.']\n",
        "  ```\n",
        "\n",
        "  Cada palabra o palabra con puntuación permanece unida porque `re.split` solo corta por espacios.\n",
        "\n",
        "Detalles adicionales:\n",
        "\n",
        "* Si el texto tuviera varios espacios seguidos, el resultado incluiría elementos vacíos (`''`) donde se encuentran separadores consecutivos.\n",
        "* Ejemplo: `'a  b'` → `['a', '', 'b']`.\n",
        "* Para evitar vacíos: `re.split(r'\\s+', text.strip())`, que divide por uno o más espacios y elimina espacios iniciales/finales.\n"
      ],
      "metadata": {
        "id": "3E7-Bb1gB19q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuVAWSDq53M0",
        "outputId": "73f816b1-1580-4c3c-9a77-51744dc29654"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', ',', '', ' ', 'you', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación detallada:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "```\n",
        "\n",
        "* `re.split(patrón, texto)` divide `texto` usando las **coincidencias del patrón** como separadores.\n",
        "\n",
        "* El patrón `r'([,.]|\\s)'` se interpreta así:\n",
        "\n",
        "  * `r'...'`: *raw string literal* evita que `\\s` sea procesado como escape por Python.\n",
        "  * `(...)`: **grupo de captura**. Si el patrón contiene paréntesis, los separadores encontrados se **conservan** en la lista de salida.\n",
        "  * `[,.]`: clase de caracteres; coincide con **una coma (`,`) o un punto (`.`)**.\n",
        "  * `|`: operador **OR** lógico en expresiones regulares.\n",
        "  * `\\s`: coincide con **cualquier espacio en blanco** (espacio, tabulador, salto de línea, etc.).\n",
        "\n",
        "* En conjunto, el patrón significa:\n",
        "  “Divide el texto cada vez que aparezca una coma, un punto o un espacio, e incluye ese carácter separador como elemento independiente en el resultado.”\n",
        "\n",
        "* El grupo de captura `(...)` es lo que diferencia este ejemplo del anterior: sin él, los separadores se descartarían; con él, se **mantienen** en la lista.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* Muestra la lista completa generada.\n",
        "* Para `text = 'Hey, you. This, is a test.'`, el resultado es:\n",
        "\n",
        "  ```\n",
        "  ['Hey', ',', '', ' ', 'you', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
        "  ```\n",
        "\n",
        "Análisis del resultado:\n",
        "\n",
        "* Cada palabra o signo está separado individualmente.\n",
        "* Los elementos vacíos (`''`) aparecen porque `re.split` puede generar subcadenas vacías cuando dos separadores se suceden (por ejemplo, una coma seguida de un espacio).\n",
        "* Los separadores detectados (`,`, `.`, `' '`) se mantienen en la lista.\n",
        "\n",
        "Uso posterior:\n",
        "\n",
        "* Estos resultados suelen limpiarse con una lista por comprensión:\n",
        "\n",
        "  ```python\n",
        "  result = [item for item in result if item.strip()]\n",
        "  ```\n",
        "\n",
        "  que elimina los vacíos y deja solo palabras y signos de puntuación separados.\n"
      ],
      "metadata": {
        "id": "dyc_-X-rD9pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V__CB8yR63b3",
        "outputId": "dc906839-fae7-4e94-b59a-c32495928d7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', ',', 'you', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exacta:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "result = [item for item in result if item.strip()]\n",
        "```\n",
        "\n",
        "* Es una **lista por comprensión** (*list comprehension*).\n",
        "* Recorre cada elemento `item` dentro de la lista `result` anterior.\n",
        "* `item.strip()` ejecuta el método `str.strip()` de Python, que:\n",
        "\n",
        "  * Elimina todos los caracteres de espacio en blanco al inicio y final del string (`' '`, `\\t`, `\\n`, etc.).\n",
        "  * Devuelve el texto sin esos espacios.\n",
        "* En un contexto booleano, un string vacío `''` evalúa como `False`.\n",
        "* Así, la condición `if item.strip()` **filtra** y mantiene solo los elementos cuyo resultado no está vacío.\n",
        "* Resultado: se eliminan todos los tokens vacíos (`''`) generados por separadores consecutivos o espacios redundantes.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* Imprime la nueva lista limpia, sin elementos vacíos.\n",
        "* Con `text = 'Hey, you. This, is a test.'`, la salida será:\n",
        "\n",
        "  ```\n",
        "  ['Hey', ',', 'you', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
        "  ```\n",
        "\n",
        "Resultado final:\n",
        "\n",
        "* Cada palabra y signo de puntuación es un token independiente.\n",
        "* Se conserva el orden original del texto.\n",
        "* Ya no hay espacios ni cadenas vacías.\n",
        "\n",
        "Importancia:\n",
        "\n",
        "* Este filtrado prepara el texto para construir vocabularios o convertir tokens en IDs numéricos, pasos previos al entrenamiento de un modelo de lenguaje.\n"
      ],
      "metadata": {
        "id": "XMiEsTBmQDG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hey, you. Is ths-- a test?'\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSf0Tw7ADtHg",
        "outputId": "25e5ee2e-21d2-4f80-fe7c-c171b30728b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', ',', 'you', '.', 'Is', 'ths', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "text = 'Hey, you. Is ths-- a test?'\n",
        "```\n",
        "\n",
        "* Define la variable `text` como una cadena (`str`).\n",
        "* Contiene palabras, signos de puntuación, un doble guion (`--`) y espacios.\n",
        "* Este tipo de texto se usa para probar una tokenización más compleja.\n",
        "\n",
        "---\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "```\n",
        "\n",
        "* Usa el método `re.split()` del módulo `re` para **dividir el texto** con una expresión regular avanzada.\n",
        "\n",
        "* Desglose del patrón `r'([,.:;?_!\"()\\']|--|\\s)'`:\n",
        "\n",
        "  * `r'...'`: *raw string literal*, evita que Python interprete `\\` como secuencia de escape.\n",
        "  * `(...)`: **grupo de captura**, conserva los separadores en la salida.\n",
        "  * `[ ,.:;?_!\"()\\' ]`: **clase de caracteres**, coincide con cualquiera de estos signos de puntuación:\n",
        "    `, . : ; ? _ ! \" ( ) '`\n",
        "  * `|--`: el operador `|` indica “o”. Aquí captura explícitamente la secuencia de **doble guion** `--`.\n",
        "  * `|\\s`: también divide donde haya **espacios en blanco** (espacios, tabuladores o saltos de línea).\n",
        "\n",
        "* En conjunto, el patrón significa:\n",
        "  “Divide el texto en cada espacio o signo de puntuación del listado, y conserva esos signos como elementos separados en la lista resultante.”\n",
        "\n",
        "---\n",
        "\n",
        "3.\n",
        "\n",
        "```python\n",
        "result = [item for item in result if item.strip()]\n",
        "```\n",
        "\n",
        "* Lista por comprensión para eliminar tokens vacíos (`''`) o compuestos solo por espacios.\n",
        "* `item.strip()` elimina espacios al inicio y al final; si el resultado está vacío, se descarta.\n",
        "* Resultado: una lista de tokens limpios (palabras y signos).\n",
        "\n",
        "---\n",
        "\n",
        "4.\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* Imprime la lista final de tokens.\n",
        "* Salida esperada:\n",
        "\n",
        "  ```\n",
        "  ['Hey', ',', 'you', '.', 'Is', 'ths', '--', 'a', 'test', '?']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen funcional:**\n",
        "\n",
        "* La expresión regular separa palabras y signos de puntuación en elementos individuales.\n",
        "* El filtrado posterior elimina espacios vacíos.\n",
        "* El resultado es una **tokenización básica** útil para construir vocabularios o convertir texto en secuencias de IDs numéricos en el preprocesamiento de un LLM.\n"
      ],
      "metadata": {
        "id": "-O6Poj9LF41M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizando el texto\n",
        "\n"
      ],
      "metadata": {
        "id": "9dYa5PE2KNZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8zBEv2NJm5t",
        "outputId": "d89d5cb8-a386-4a36-811e-8f868de887b1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### 1)\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Tokenizar el texto completo cargado en `raw_text` (por ejemplo, el cuento *The Verdict*).\n",
        "Divide el texto en unidades mínimas (*tokens*) —palabras y signos de puntuación— que luego se usarán como entrada para un modelo de lenguaje.\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* **`re.split(patrón, cadena)`**\n",
        "  Función del módulo estándar `re` (*regular expressions*) que divide una cadena (`cadena`) en una lista, usando las coincidencias del `patrón` como delimitadores.\n",
        "  A diferencia de `str.split()`, que solo separa por espacios, `re.split()` puede usar expresiones regulares complejas y permite conservar los delimitadores si están dentro de **paréntesis de captura**.\n",
        "\n",
        "* **`r'([,.:;?_!\"()\\']|--|\\s)'`**\n",
        "  Es un *raw string literal* (`r'...'`), lo que evita que Python interprete `\\` como secuencia de escape (por ejemplo, `\\s` no se convierte en “espacio”, sino que se pasa literalmente a la expresión regular).\n",
        "\n",
        "  Desglose del patrón:\n",
        "\n",
        "  * **`(...)`** → Grupo de captura. Todo lo que coincida dentro de este grupo se incluye en la lista de salida.\n",
        "  * **`[,.:;?_!\"()\\' ]`** → Clase de caracteres que captura **cualquiera** de los signos de puntuación listados:\n",
        "    `, . : ; ? _ ! \" ( ) '`\n",
        "  * **`|--`** → Alternativa literal que detecta la secuencia exacta de **doble guion** `--` (frecuente en narrativa).\n",
        "  * **`|\\s`** → Detecta cualquier carácter de espacio en blanco (`' '`, `\\t`, `\\n`, etc.).\n",
        "\n",
        "  En conjunto, el patrón significa:\n",
        "\n",
        "  > Divide el texto cada vez que aparezca una coma, punto, signo de interrogación, doble guion o espacio, e incluye esos separadores en la lista resultante.\n",
        "\n",
        "**Resultado intermedio:**\n",
        "La variable `preprocessed` se convierte en una lista en la que cada elemento puede ser:\n",
        "\n",
        "* una palabra,\n",
        "* un signo de puntuación aislado (`,`, `.`, `--`, etc.), o\n",
        "* una cadena vacía si se encuentran separadores consecutivos.\n",
        "\n",
        "---\n",
        "\n",
        "### 2)\n",
        "\n",
        "```python\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Limpiar la lista de tokens eliminando elementos vacíos o compuestos solo por espacios.\n",
        "\n",
        "**Explicación técnica:**\n",
        "\n",
        "* `item.strip()` ejecuta el método `str.strip()` sobre cada `item`:\n",
        "\n",
        "  * Quita caracteres de espacio en blanco al inicio y final del string.\n",
        "  * Devuelve una nueva cadena sin esos caracteres.\n",
        "* En un contexto booleano, un string vacío (`''`) se evalúa como `False`.\n",
        "* Por tanto, la condición `if item.strip()` **filtra** y conserva solo los elementos con contenido textual o de puntuación válido.\n",
        "* El resultado final es una lista limpia con todos los tokens “reales” que el modelo procesará.\n",
        "\n",
        "**Complejidad temporal:**\n",
        "O(n) respecto al número de tokens, ya que el filtrado recorre cada elemento una vez.\n",
        "\n",
        "---\n",
        "\n",
        "### 3)\n",
        "\n",
        "```python\n",
        "print(len(preprocessed))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Mostrar cuántos tokens (palabras y signos) contiene el texto tras la tokenización y limpieza.\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `len(preprocessed)` devuelve el número de elementos de la lista (un entero).\n",
        "* Este número equivale a la **longitud del corpus tokenizado**.\n",
        "* Permite comprobar si la división del texto produjo una cantidad razonable de tokens.\n",
        "\n",
        "  * Por ejemplo, el cuento *The Verdict* produce ~4 690 tokens sin espacios.\n",
        "\n",
        "**Consideraciones de uso:**\n",
        "\n",
        "* Este conteo no equivale a palabras “semánticas”, ya que incluye signos de puntuación como tokens separados.\n",
        "* El resultado sirve para determinar el tamaño del vocabulario base que se usará en la siguiente etapa (construcción del diccionario de tokens únicos).\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen funcional global:**\n",
        "\n",
        "1. Divide el texto en palabras y signos mediante una expresión regular amplia.\n",
        "2. Elimina vacíos y espacios innecesarios.\n",
        "3. Devuelve una lista limpia de tokens que representa la forma mínima procesable del corpus textual.\n",
        "4. Muestra el número total de tokens para verificar la segmentación antes de construir el vocabulario del modelo.\n"
      ],
      "metadata": {
        "id": "nY8ESHqiHHXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Convirtiendo tokens en ID's"
      ],
      "metadata": {
        "id": "T0FNOVz5KR_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)\n",
        "\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
        "\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frSDUAJOKeIr",
        "outputId": "3d177700-c0bf-4306-b9ab-79057cad6554"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n",
            "('His', 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### 1)\n",
        "\n",
        "```python\n",
        "all_words = sorted(set(preprocessed))\n",
        "```\n",
        "\n",
        "**Objetivo:**\n",
        "Extraer el **vocabulario único** del corpus tokenizado `preprocessed` y ordenarlo alfabéticamente.\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* `set(preprocessed)`\n",
        "\n",
        "  * Convierte la lista `preprocessed` (que contiene tokens repetidos) en un **conjunto** (`set`), una estructura de datos desordenada y sin duplicados.\n",
        "  * Cada elemento del conjunto es **único**, por lo que esta operación elimina repeticiones.\n",
        "  * Implementación interna: los `set` de Python están basados en **tablas hash**.\n",
        "\n",
        "    * La inserción y la verificación de unicidad (`hashing`) tienen complejidad media **O(1)**.\n",
        "  * Complejidad total de conversión: **O(n)**, donde *n* es el número de tokens en `preprocessed`.\n",
        "\n",
        "* `sorted(...)`\n",
        "\n",
        "  * Toma el conjunto resultante y devuelve una **lista ordenada lexicográficamente**.\n",
        "  * La ordenación se realiza con el algoritmo **Timsort** (fusión adaptativa, estable y de complejidad O(n log n)).\n",
        "  * El orden lexicográfico depende de los códigos Unicode de los caracteres, por lo que letras mayúsculas (`'A'`) aparecen antes que minúsculas (`'a'`).\n",
        "\n",
        "**Resultado:**\n",
        "Una lista (`list`) llamada `all_words` que contiene todos los **tokens únicos** (palabras y signos de puntuación), ordenados.\n",
        "\n",
        "---\n",
        "\n",
        "### 2)\n",
        "\n",
        "```python\n",
        "vocab_size = len(all_words)\n",
        "```\n",
        "\n",
        "**Objetivo:**\n",
        "Calcular el tamaño del vocabulario, es decir, cuántos tokens distintos existen.\n",
        "\n",
        "**Detalles:**\n",
        "\n",
        "* `len()` accede al atributo de longitud de la lista (`PyObject_VAR_HEAD->ob_size` en CPython).\n",
        "* Complejidad temporal: **O(1)**, ya que `len()` no recorre la lista; accede directamente al valor almacenado internamente.\n",
        "* `vocab_size` es un entero (`int`) que representa el número total de tokens únicos.\n",
        "\n",
        "---\n",
        "\n",
        "### 3)\n",
        "\n",
        "```python\n",
        "print(vocab_size)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Mostrar la cantidad de elementos únicos (palabras, signos y símbolos) en el vocabulario.\n",
        "\n",
        "* En el ejemplo del libro, para *The Verdict*, este valor es aproximadamente **1 130** tokens.\n",
        "* Este número será fundamental para:\n",
        "\n",
        "  * definir el tamaño de las matrices de embeddings,\n",
        "  * establecer la dimensionalidad de las capas de entrada y salida del modelo (por ejemplo, `nn.Embedding(vocab_size, embed_dim)` en PyTorch).\n",
        "\n",
        "---\n",
        "\n",
        "### 4)\n",
        "\n",
        "```python\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
        "```\n",
        "\n",
        "**Objetivo:**\n",
        "Crear un **diccionario de mapeo** entre tokens y sus identificadores numéricos (*token IDs*).\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* **`enumerate(all_words)`**\n",
        "\n",
        "  * Genera un iterador que produce tuplas `(índice, elemento)` para cada token en `all_words`.\n",
        "  * Por defecto, la enumeración comienza en 0.\n",
        "  * Ejemplo: `[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), ...]`.\n",
        "  * Complejidad: **O(n)**, donde *n* es el número de tokens únicos.\n",
        "\n",
        "* **Comprensión de diccionario `{token: integer for integer, token in enumerate(all_words)}`**\n",
        "\n",
        "  * Recorre cada par `(integer, token)` producido por `enumerate`.\n",
        "  * Asigna el token (`token`) como **clave** y el número (`integer`) como **valor**.\n",
        "  * Resultado: un diccionario que implementa la función ( f : \\text{token} \\mapsto \\text{ID entero} ).\n",
        "\n",
        "**Propiedades del diccionario (`dict`):**\n",
        "\n",
        "* En Python 3.7+, los diccionarios **mantienen el orden de inserción**, por lo que el orden coincide con la lista ordenada `all_words`.\n",
        "* Internamente usan **tablas hash** (resolución abierta), lo que permite:\n",
        "\n",
        "  * Búsqueda promedio: **O(1)**.\n",
        "  * Inserción promedio: **O(1)**.\n",
        "* Tamaño de `vocab` = `vocab_size`.\n",
        "\n",
        "**Importancia:**\n",
        "Este mapeo es el núcleo del proceso de **tokenización numérica**, que permite convertir texto en tensores de enteros antes del entrenamiento del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "### 5)\n",
        "\n",
        "```python\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break\n",
        "```\n",
        "\n",
        "**Objetivo:**\n",
        "Verificar visualmente las primeras asignaciones del diccionario `vocab`.\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* **`vocab.items()`**\n",
        "\n",
        "  * Devuelve un objeto *view* que itera sobre las **tuplas (clave, valor)** del diccionario.\n",
        "  * Ejemplo: `('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), ...`.\n",
        "  * Complejidad: **O(1)** por acceso, **O(n)** por recorrido completo.\n",
        "\n",
        "* **`enumerate(...)`**\n",
        "\n",
        "  * Agrega un contador `i` para poder detener el bucle después de imprimir 51 pares.\n",
        "\n",
        "* **`print(item)`**\n",
        "\n",
        "  * Muestra en consola cada par `(token, id)`.\n",
        "\n",
        "* **Condición `if i > 50: break`**\n",
        "\n",
        "  * Interrumpe el bucle tras 51 iteraciones (índices de 0 a 50).\n",
        "  * Esto evita imprimir el vocabulario completo, que puede tener cientos o miles de tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen funcional completo:**\n",
        "\n",
        "1. Se eliminan duplicados del corpus tokenizado.\n",
        "2. Se ordenan los tokens para generar una lista estable y reproducible (`all_words`).\n",
        "3. Se calcula el tamaño del vocabulario (`vocab_size`).\n",
        "4. Se crea un diccionario `vocab` que asigna a cada token un identificador entero único, base de la codificación del texto.\n",
        "5. Se imprime una muestra del mapeo para validación visual.\n",
        "\n",
        "---\n",
        "\n",
        "**Resultado conceptual:**\n",
        "Este bloque construye la **primera capa de representación simbólica** de un LLM:\n",
        "\n",
        "> un espacio discreto de vocabulario donde cada símbolo del lenguaje natural queda asociado a un índice entero, preparando el terreno para el embedding vectorial que convertirá estos índices en representaciones continuas en la siguiente etapa del pipeline.\n"
      ],
      "metadata": {
        "id": "vldhOgOlHbG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()} # ('his', 51) i= 51, s= his\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = ' '.join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "x02SO6__M7A5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva del código y de los principios que implementa:\n",
        "\n",
        "---\n",
        "\n",
        "## **Clase `SimpleTokenizerV1`**\n",
        "\n",
        "Esta clase define un **tokenizador mínimo y bidireccional** que convierte texto en secuencias numéricas (*encoding*) y viceversa (*decoding*).\n",
        "Opera sobre un vocabulario (`vocab`) previamente construido que asigna **tokens → IDs numéricos**.\n",
        "\n",
        "La implementación ilustra el flujo esencial de un **preprocesador de texto para LLMs** antes de usar técnicas más avanzadas como *byte pair encoding (BPE)*.\n",
        "\n",
        "---\n",
        "\n",
        "### **1) Definición de clase**\n",
        "\n",
        "```python\n",
        "class SimpleTokenizerV1:\n",
        "```\n",
        "\n",
        "* Define un **objeto de tipo clase** en Python que agrupa datos (atributos) y comportamiento (métodos).\n",
        "* Sirve como plantilla para crear instancias específicas del tokenizador.\n",
        "* En tiempo de ejecución, cada instancia mantiene su propio estado interno (diccionarios de mapeo, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### **2) Método `__init__`**\n",
        "\n",
        "```python\n",
        "def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}  # ('his', 51) → i=51, s='his'\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Inicializa la instancia y crea los dos diccionarios de mapeo complementarios:\n",
        "\n",
        "1. **`str_to_int`**: de token (texto) a ID (entero).\n",
        "2. **`int_to_str`**: de ID (entero) a token (texto).\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `__init__` es el **constructor** especial de Python. Se ejecuta automáticamente al crear un nuevo objeto:\n",
        "\n",
        "  ```python\n",
        "  tokenizer = SimpleTokenizerV1(vocab)\n",
        "  ```\n",
        "\n",
        "  Este llama implícitamente a `SimpleTokenizerV1.__init__(tokenizer, vocab)`.\n",
        "\n",
        "* `self.str_to_int = vocab`\n",
        "\n",
        "  * Almacena el diccionario que mapea cada token único del vocabulario a un entero (ID).\n",
        "  * Ejemplo: `{'!', 0, '\"': 1, \"'\", 2, ..., 'his': 51}`.\n",
        "\n",
        "* `self.int_to_str = {i: s for s, i in vocab.items()}`\n",
        "\n",
        "  * **Inversión del mapeo.**\n",
        "  * `vocab.items()` produce pares `(token, id)`.\n",
        "  * La comprensión de diccionario los invierte para formar `(id, token)`.\n",
        "  * Ejemplo: `{51: 'his', 52: 'her', ...}`.\n",
        "  * Esto permite reconstruir texto desde secuencias de IDs en la operación *decode*.\n",
        "  * Complejidad de construcción: **O(n)** en el número de tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### **3) Método `encode`**\n",
        "\n",
        "```python\n",
        "def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "```\n",
        "\n",
        "**Función general:**\n",
        "Convierte texto plano en una secuencia de **identificadores numéricos** según el vocabulario.\n",
        "\n",
        "**Etapas detalladas:**\n",
        "\n",
        "1. **Tokenización inicial**\n",
        "\n",
        "   ```python\n",
        "   preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "   ```\n",
        "\n",
        "   * Usa una expresión regular para dividir el texto en **palabras y signos de puntuación**.\n",
        "   * Patrón:\n",
        "\n",
        "     * `(...)`: grupo de captura (mantiene los delimitadores).\n",
        "     * `[,.?_!\"()']`: signos a separar.\n",
        "     * `|--`: doble guion literal.\n",
        "     * `|\\s`: espacio o tabulador.\n",
        "   * Ejemplo:\n",
        "     `'Hey, you!' → ['Hey', ',', '', ' ', 'you', '!', '']`\n",
        "\n",
        "2. **Limpieza de tokens**\n",
        "\n",
        "   ```python\n",
        "   preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "   ```\n",
        "\n",
        "   * `str.strip()` elimina espacios en blanco al inicio y fin de cada token.\n",
        "   * La condición `if item.strip()` elimina elementos vacíos (`''`).\n",
        "   * Resultado limpio: `['Hey', ',', 'you', '!']`.\n",
        "\n",
        "3. **Conversión a IDs**\n",
        "\n",
        "   ```python\n",
        "   ids = [self.str_to_int[s] for s in preprocessed]\n",
        "   ```\n",
        "\n",
        "   * Sustituye cada token textual por su ID numérico.\n",
        "   * Acceso al diccionario: O(1) promedio por búsqueda (tabla hash).\n",
        "   * Si un token no está en `vocab`, esta versión **lanza KeyError** (no maneja desconocidos).\n",
        "   * Ejemplo:\n",
        "     Si `vocab['Hey'] = 45`, `vocab[','] = 3`, la salida será `[45, 3, 120, 5]`.\n",
        "\n",
        "4. **Retorno**\n",
        "\n",
        "   * Devuelve una lista de enteros (`List[int]`).\n",
        "   * Este formato es compatible con frameworks como PyTorch (`torch.tensor(ids)`).\n",
        "\n",
        "---\n",
        "\n",
        "### **4) Método `decode`**\n",
        "\n",
        "```python\n",
        "def decode(self, ids):\n",
        "    text = ' '.join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "**Función general:**\n",
        "Transforma una secuencia numérica de tokens de vuelta a texto legible.\n",
        "\n",
        "**Etapas detalladas:**\n",
        "\n",
        "1. **Conversión de IDs a texto**\n",
        "\n",
        "   ```python\n",
        "   [self.int_to_str[i] for i in ids]\n",
        "   ```\n",
        "\n",
        "   * Usa el diccionario inverso `int_to_str` para reconstruir los tokens.\n",
        "   * Genera una lista de strings.\n",
        "   * Si un ID no está en el diccionario, se lanzará un `KeyError`.\n",
        "\n",
        "2. **Concatenación con espacios**\n",
        "\n",
        "   ```python\n",
        "   ' '.join([...])\n",
        "   ```\n",
        "\n",
        "   * Une todos los tokens con un **espacio** entre ellos.\n",
        "   * Ejemplo: `['Hey', ',', 'you', '!'] → 'Hey , you !'`.\n",
        "\n",
        "3. **Corrección de espaciado antes de signos**\n",
        "\n",
        "   ```python\n",
        "   re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "   ```\n",
        "\n",
        "   * Busca espacios seguidos de signos de puntuación.\n",
        "   * Patrón:\n",
        "\n",
        "     * `\\s+`: uno o más espacios.\n",
        "     * `([,.?!\"()'])`: grupo de captura con signos.\n",
        "   * Reemplazo `r'\\1'`: mantiene solo el signo, eliminando el espacio anterior.\n",
        "   * Resultado: `'Hey, you!'` (sin espacio extra antes de `,` o `!`).\n",
        "   * Complejidad: O(n) sobre la longitud del texto.\n",
        "\n",
        "4. **Retorno**\n",
        "\n",
        "   * Devuelve el texto corregido y reconstruido como `str`.\n",
        "\n",
        "---\n",
        "\n",
        "### **5) Ejemplo de uso**\n",
        "\n",
        "```python\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"It's the last he painted, you know.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "print(tokenizer.decode(ids))\n",
        "```\n",
        "\n",
        "**Flujo interno:**\n",
        "\n",
        "1. `encode()` → tokeniza → mapea a IDs.\n",
        "2. `decode()` → convierte IDs → reconstruye texto.\n",
        "\n",
        "**Resultado esperado:**\n",
        "\n",
        "```python\n",
        "[12, 45, 78, 91, 8, 120, 5, 32, 14]  # ejemplo de IDs\n",
        "\"It's the last he painted, you know.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **6) Limitaciones del diseño V1**\n",
        "\n",
        "* No maneja **palabras desconocidas**: si un token no está en el vocabulario, lanza `KeyError`.\n",
        "* No incluye **tokens especiales** (`<|unk|>`, `<|endoftext|>`, etc.).\n",
        "* No conserva espacios exactos, por lo que pierde formato o saltos de línea.\n",
        "* Sirve como **prototipo conceptual** antes de evolucionar hacia `SimpleTokenizerV2` y finalmente hacia el **BPE tokenizer** usado en GPT.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen conceptual**\n",
        "\n",
        "| Etapa    | Entrada      | Proceso                             | Salida        | Complejidad |\n",
        "| -------- | ------------ | ----------------------------------- | ------------- | ----------- |\n",
        "| `encode` | texto crudo  | regex + limpieza + mapeo            | lista de IDs  | O(n)        |\n",
        "| `decode` | lista de IDs | reconstrucción + ajuste de espacios | texto legible | O(n)        |\n",
        "\n",
        "El objetivo pedagógico de `SimpleTokenizerV1` es mostrar **cómo un LLM traduce lenguaje natural a un espacio discreto numérico** antes de aplicar embeddings y atención. Es la base de la representación simbólica que alimenta las capas iniciales del modelo transformer.\n"
      ],
      "metadata": {
        "id": "igstkADsHqwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"It's the last he painted, you know, \"Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxJmX_pez3vB",
        "outputId": "49889fe4-fe39-422f-d5a6-f4ca52710491"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Crea una **instancia** de la clase `SimpleTokenizerV1` definida previamente, usando como entrada el diccionario `vocab`.\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `SimpleTokenizerV1(vocab)` ejecuta internamente:\n",
        "\n",
        "  ```python\n",
        "  SimpleTokenizerV1.__init__(tokenizer, vocab)\n",
        "  ```\n",
        "\n",
        "  donde `tokenizer` es el nuevo objeto creado en memoria (tipo `SimpleTokenizerV1`).\n",
        "\n",
        "* En el constructor (`__init__`):\n",
        "\n",
        "  * `self.str_to_int = vocab` asigna el vocabulario base (tokens → IDs).\n",
        "  * `self.int_to_str = {i: s for s, i in vocab.items()}` crea el diccionario inverso (IDs → tokens).\n",
        "\n",
        "**Estructura interna resultante:**\n",
        "\n",
        "* `tokenizer.str_to_int` → `{'!': 0, '\"': 1, ... 'painted': 418, ...}`\n",
        "* `tokenizer.int_to_str` → `{0: '!', 1: '\"', ... 418: 'painted', ...}`\n",
        "\n",
        "Estas estructuras residen en la memoria heap y son accedidas por referencia a través del objeto `tokenizer`.\n",
        "\n",
        "**Complejidad:**\n",
        "\n",
        "* Construcción del diccionario inverso: **O(n)** donde *n* es el tamaño del vocabulario.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "text = \"\"\"Its the last he painted, you know, \"Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Define el texto que será convertido en tokens y luego a IDs.\n",
        "\n",
        "**Detalles:**\n",
        "\n",
        "* Uso de triple comillas `\"\"\"...\"\"\"` permite escribir cadenas multilínea sin necesidad de escapes.\n",
        "  Aquí se usa en una sola línea, pero sigue siendo válido.\n",
        "* El texto incluye:\n",
        "\n",
        "  * palabras (`Its`, `the`, `last`, `painted`, `know`, `said`, etc.),\n",
        "  * signos de puntuación (`,`, `\"`, `.`),\n",
        "  * un nombre propio con mayúscula (`Mrs. Gisburn`).\n",
        "\n",
        "**Nota importante:**\n",
        "El token `\"Its\"` (sin apóstrofe) podría **no existir en el vocabulario** si el texto de entrenamiento original solo contenía `\"It's\"`.\n",
        "Dado que `SimpleTokenizerV1` **no maneja palabras desconocidas**, esto puede causar un `KeyError`.\n",
        "\n",
        "---\n",
        "\n",
        "### **3)**\n",
        "\n",
        "```python\n",
        "ids = tokenizer.encode(text)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Convierte el texto en una secuencia de enteros (*token IDs*) siguiendo el vocabulario cargado.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Etapas internas del método `encode`:**\n",
        "\n",
        "1. **Tokenización inicial**\n",
        "\n",
        "   ```python\n",
        "   preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "   ```\n",
        "\n",
        "   * Divide el texto en palabras, puntuaciones y espacios.\n",
        "   * Cada separador se conserva como elemento separado.\n",
        "   * Ejemplo intermedio:\n",
        "\n",
        "     ```\n",
        "     ['Its', ' ', 'the', ' ', 'last', ' ', 'he', ' ', 'painted', ',', ' ', 'you', ' ', 'know', ',', ' ', '\"', 'Mrs', '.', ' ', 'Gisburn', ' ', 'said', ' ', 'with', ' ', 'pardonable', ' ', 'pride', '.', '', '']\n",
        "     ```\n",
        "\n",
        "2. **Limpieza**\n",
        "\n",
        "   ```python\n",
        "   preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "   ```\n",
        "\n",
        "   * Elimina cadenas vacías y espacios redundantes.\n",
        "   * Resultado limpio:\n",
        "\n",
        "     ```\n",
        "     ['Its', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n",
        "     ```\n",
        "\n",
        "3. **Conversión a IDs**\n",
        "\n",
        "   ```python\n",
        "   ids = [self.str_to_int[s] for s in preprocessed]\n",
        "   ```\n",
        "\n",
        "   * Busca cada token en el diccionario `str_to_int`.\n",
        "   * Si todos los tokens están presentes en `vocab`, se obtiene una lista de enteros:\n",
        "\n",
        "     ```\n",
        "     [315, 27, 46, 89, 418, 3, 57, 108, 3, 1, 302, 7, 410, 85, 92, 600, 612, 7]\n",
        "     ```\n",
        "\n",
        "     *(Los valores son ilustrativos; dependen del vocabulario real.)*\n",
        "   * Si un token no existe (p. ej., `\"Its\"` en lugar de `\"It's\"`), Python lanzará:\n",
        "\n",
        "     ```\n",
        "     KeyError: 'Its'\n",
        "     ```\n",
        "\n",
        "**Razonamiento del error:**\n",
        "\n",
        "* El vocabulario proviene del cuento *The Verdict*, que probablemente no contiene `\"Its\"`.\n",
        "* La versión `\"It's\"` (con apóstrofe) sí estaría registrada como un token distinto.\n",
        "* Por tanto, el tokenizador V1 falla con palabras fuera del conjunto de entrenamiento.\n",
        "\n",
        "**Complejidad:**\n",
        "\n",
        "* Búsqueda en diccionario para *n* tokens → **O(n)** promedio, ya que cada acceso hash es O(1).\n",
        "\n",
        "---\n",
        "\n",
        "### **4)**\n",
        "\n",
        "```python\n",
        "print(ids)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Muestra la lista numérica resultante, útil para validar la conversión.\n",
        "\n",
        "* Si todos los tokens existen: imprime los **índices numéricos** del texto.\n",
        "  Ejemplo hipotético:\n",
        "\n",
        "  ```\n",
        "  [315, 27, 46, 89, 418, 3, 57, 108, 3, 1, 302, 7, 410, 85, 92, 600, 612, 7]\n",
        "  ```\n",
        "* Si hay palabras desconocidas: no imprime nada y muestra un **KeyError** interrumpiendo la ejecución.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusión operativa**\n",
        "\n",
        "| Paso                       | Acción                    | Resultado                     | Complejidad |\n",
        "| -------------------------- | ------------------------- | ----------------------------- | ----------- |\n",
        "| `SimpleTokenizerV1(vocab)` | Crea mapeos token↔ID      | Diccionarios en memoria       | O(n)        |\n",
        "| `encode(text)`             | Divide, limpia, convierte | Lista de IDs                  | O(m)        |\n",
        "| `print(ids)`               | Visualiza los IDs         | Validación de la codificación | O(m)        |\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitación clave evidenciada**\n",
        "\n",
        "El **modelo V1 no soporta palabras desconocidas**.\n",
        "Por tanto, si `KeyError: 'Its'` ocurre, se debe usar `SimpleTokenizerV2`, que reemplaza tokens no vistos con `<|unk|>` (token de desconocido), asegurando compatibilidad con cualquier texto de entrada.\n"
      ],
      "metadata": {
        "id": "f-UzhbqPILt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbr4q_A61Q6J",
        "outputId": "29d3b0dd-cff1-4a27-9d26-26891b821637"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "print(tokenizer.decode(ids))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Reconstruir texto legible a partir de la secuencia de identificadores numéricos `ids` obtenidos con `encode()`, y mostrarlo en pantalla.\n",
        "\n",
        "---\n",
        "\n",
        "## **Desglose interno del método `decode()`**\n",
        "\n",
        "El método definido en la clase `SimpleTokenizerV1`:\n",
        "\n",
        "```python\n",
        "def decode(self, ids):\n",
        "    text = ' '.join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "### **Etapa 1 — Reconversión de IDs a tokens**\n",
        "\n",
        "```python\n",
        "[self.int_to_str[i] for i in ids]\n",
        "```\n",
        "\n",
        "* Usa el diccionario inverso `int_to_str`, construido durante la inicialización:\n",
        "\n",
        "  ```python\n",
        "  self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "  ```\n",
        "* Para cada entero `i` en la lista `ids`, obtiene el token textual original asociado.\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "```\n",
        "ids = [315, 27, 46, 89, 418, 3, 57, 108, 3, 1, 302, 7, 410, 85, 92, 600, 612, 7]\n",
        "↓\n",
        "['Its', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n",
        "```\n",
        "\n",
        "* Si algún ID no está en el diccionario (poco probable, ya que `encode()` solo usa IDs válidos), Python lanza `KeyError`.\n",
        "\n",
        "* Complejidad temporal: **O(n)** en el número de IDs.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Unión de tokens con espacios**\n",
        "\n",
        "```python\n",
        "text = ' '.join([...])\n",
        "```\n",
        "\n",
        "* Concatena todos los tokens, separándolos con un **espacio**.\n",
        "* El resultado inicial conserva un espacio entre cada token, incluso antes de los signos de puntuación.\n",
        "\n",
        "**Ejemplo intermedio:**\n",
        "\n",
        "```\n",
        "'Its the last he painted , you know , \" Mrs . Gisburn said with pardonable pride .'\n",
        "```\n",
        "\n",
        "* Esta forma contiene **espacios no naturales** antes de comas, puntos o comillas.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Corrección tipográfica de espacios**\n",
        "\n",
        "```python\n",
        "text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "```\n",
        "\n",
        "**Análisis de la expresión regular:**\n",
        "\n",
        "* `re.sub(patrón, reemplazo, texto)` busca todas las coincidencias de `patrón` y las sustituye por `reemplazo`.\n",
        "\n",
        "* Patrón: `r'\\s+([,.?!\"()\\'])'`\n",
        "\n",
        "  * `\\s+` → uno o más espacios.\n",
        "  * `([,.?!\"()'])` → grupo de captura que coincide con cualquier signo de puntuación listado:\n",
        "    coma, punto, interrogación, exclamación, comillas, paréntesis o apóstrofe.\n",
        "\n",
        "* Reemplazo: `r'\\1'`\n",
        "\n",
        "  * Sustituye toda la coincidencia (espacios + signo) por **solo el signo**, manteniendo lo que se capturó entre paréntesis.\n",
        "  * Resultado: elimina los espacios que preceden a la puntuación.\n",
        "\n",
        "**Ejemplo de transformación:**\n",
        "\n",
        "```\n",
        "'Its the last he painted , you know , \" Mrs . Gisburn said with pardonable pride .'\n",
        "↓\n",
        "'Its the last he painted, you know, \"Mrs. Gisburn said with pardonable pride.'\n",
        "```\n",
        "\n",
        "**Resultado:**\n",
        "\n",
        "* Se eliminan espacios antes de los signos, pero se conservan los espacios correctos entre palabras.\n",
        "* El texto recupera una forma natural y gramaticalmente correcta.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Retorno y visualización**\n",
        "\n",
        "```python\n",
        "return text\n",
        "```\n",
        "\n",
        "* Devuelve la cadena corregida al punto de llamada.\n",
        "* `print(tokenizer.decode(ids))` muestra el resultado final.\n",
        "\n",
        "**Salida esperada:**\n",
        "\n",
        "```\n",
        "Its the last he painted, you know, \"Mrs. Gisburn said with pardonable pride.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Resumen de flujo de datos**\n",
        "\n",
        "| Fase | Entrada                  | Operación                               | Salida                         | Complejidad |\n",
        "| ---- | ------------------------ | --------------------------------------- | ------------------------------ | ----------- |\n",
        "| 1    | `ids` (lista de enteros) | Mapea IDs → tokens                      | lista de strings               | O(n)        |\n",
        "| 2    | lista de tokens          | Une con `' '.join()`                    | texto con espacios redundantes | O(n)        |\n",
        "| 3    | texto con espacios       | `re.sub()` elimina espacios incorrectos | texto limpio                   | O(n)        |\n",
        "| 4    | texto limpio             | `print()`                               | visualización                  | O(1)        |\n",
        "\n",
        "---\n",
        "\n",
        "## **Limitación funcional**\n",
        "\n",
        "* Si algún token ID no tiene correspondencia en `int_to_str`, el método genera un error.\n",
        "* `SimpleTokenizerV1` **no distingue** entre comillas de apertura y cierre, ni conserva saltos de línea o tabulaciones originales.\n",
        "* No maneja tokens especiales como `<|endoftext|>` o `<|unk|>`.\n",
        "* El resultado textual está diseñado para **coherencia lingüística básica**, no para reconstrucción exacta de formato.\n",
        "\n",
        "---\n",
        "\n",
        "**En síntesis:**\n",
        "`decode()` implementa el paso inverso de `encode()` y ejemplifica el concepto de **reversibilidad parcial de tokenización**: a partir de una secuencia de índices numéricos, se puede recrear texto humano-legible mediante un mapeo inverso y una corrección superficial de espacios, etapa fundamental antes de evaluar la calidad de generación de un LLM.\n"
      ],
      "metadata": {
        "id": "fwp7nl5xIwlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aquí esperamos un error:\n",
        "\n",
        "text = 'Hi, do you lik tea?' # Hi es una palabra no contenida en el vocabulario generado en lso pasos anteriores\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "92gOCNBY2CEj",
        "outputId": "6b258f02-e240-4f89-b1b2-d00077cc3be4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-261838254.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Hi, do you lik tea?'\u001b[0m \u001b[0;31m# Hi es una palabra no contenida en el vocabulario generado en lso pasos anteriores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2492665210.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hi'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "text = 'Hi, do you lik tea?'\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Definir una nueva cadena de texto para comprobar el comportamiento del tokenizador cuando aparecen **palabras fuera del vocabulario**.\n",
        "\n",
        "**Contexto técnico:**\n",
        "El vocabulario `vocab` se generó a partir del cuento *The Verdict* de Edith Wharton.\n",
        "Ese corpus no contiene todas las palabras del inglés; por ejemplo, es probable que no incluya:\n",
        "\n",
        "* `\"Hi\"` (saludo coloquial),\n",
        "* `\"lik\"` (error ortográfico de `\"like\"`).\n",
        "\n",
        "Por tanto, estas palabras estarán **fuera del conjunto de tokens conocidos**.\n",
        "\n",
        "**Implicación:**\n",
        "El tokenizador actual (`SimpleTokenizerV1`) no maneja “out-of-vocabulary” (OOV) tokens.\n",
        "Intentar convertir un token desconocido a su ID provocará una excepción de tipo `KeyError`.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "print(tokenizer.encode(text))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Llamar al método `encode()` para tokenizar y convertir el texto en IDs numéricos, e imprimir el resultado.\n",
        "\n",
        "---\n",
        "\n",
        "## **Flujo interno dentro de `encode()`**\n",
        "\n",
        "```python\n",
        "def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 1 — División del texto**\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "```\n",
        "\n",
        "* Usa una expresión regular para dividir el texto por signos de puntuación y espacios, manteniendo los separadores.\n",
        "* Resultado intermedio aproximado:\n",
        "\n",
        "  ```\n",
        "  ['Hi', ',', '', ' ', 'do', ' ', 'you', ' ', 'lik', ' ', 'tea', '?', '']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Limpieza**\n",
        "\n",
        "```python\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "```\n",
        "\n",
        "* Elimina espacios y cadenas vacías, dejando solo tokens válidos.\n",
        "* Resultado limpio:\n",
        "\n",
        "  ```\n",
        "  ['Hi', ',', 'do', 'you', 'lik', 'tea', '?']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Conversión a IDs**\n",
        "\n",
        "```python\n",
        "ids = [self.str_to_int[s] for s in preprocessed]\n",
        "```\n",
        "\n",
        "* Intenta acceder a cada token dentro del diccionario `self.str_to_int` (el vocabulario).\n",
        "* El tokenizador busca:\n",
        "\n",
        "  * `vocab['Hi']`\n",
        "  * `vocab[',']`\n",
        "  * `vocab['do']`\n",
        "  * `vocab['you']`\n",
        "  * `vocab['lik']`\n",
        "  * `vocab['tea']`\n",
        "  * `vocab['?']`\n",
        "\n",
        "**Resultado técnico:**\n",
        "\n",
        "* Si **todos** los tokens están presentes, devuelve una lista de enteros (`List[int]`).\n",
        "* En este caso, `\"Hi\"` y `\"lik\"` **no existen** en el vocabulario.\n",
        "* En cuanto el intérprete intenta acceder a `self.str_to_int['Hi']`, el diccionario lanza:\n",
        "\n",
        "  ```python\n",
        "  KeyError: 'Hi'\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Impresión**\n",
        "\n",
        "* La ejecución se interrumpe antes de llegar a `print()`.\n",
        "* Python muestra la traza del error:\n",
        "\n",
        "  ```\n",
        "  KeyError: 'Hi'\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## **Análisis del error**\n",
        "\n",
        "**Tipo de error:**\n",
        "`KeyError` — ocurre cuando se intenta acceder a una clave inexistente en un diccionario.\n",
        "\n",
        "**Razón interna:**\n",
        "\n",
        "* En `SimpleTokenizerV1`, el método `encode()` no incluye ningún manejo de excepciones.\n",
        "* El acceso directo `self.str_to_int[s]` depende de que todos los tokens estén presentes.\n",
        "* Python, al no encontrar la clave `'Hi'` en la tabla hash interna del diccionario `self.str_to_int`, lanza la excepción.\n",
        "\n",
        "**Comportamiento esperado (salida):**\n",
        "\n",
        "```\n",
        "KeyError: 'Hi'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Solución conceptual (adelanto del siguiente paso del libro)**\n",
        "\n",
        "Para manejar palabras desconocidas, se usa una versión extendida del tokenizador:\n",
        "\n",
        "```python\n",
        "class SimpleTokenizerV2:\n",
        "    def encode(self, text):\n",
        "        ...\n",
        "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "```\n",
        "\n",
        "* Introduce un **token especial `<|unk|>`** (*unknown token*).\n",
        "* Cada palabra fuera del vocabulario se reemplaza automáticamente por ese símbolo.\n",
        "* Así, el método nunca falla y puede codificar cualquier texto.\n",
        "\n",
        "**Ejemplo con `SimpleTokenizerV2`:**\n",
        "\n",
        "```\n",
        "text = 'Hi, do you lik tea?'\n",
        "↓\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?']\n",
        "↓\n",
        "[1160, 3, 242, 57, 1160, 642, 10]\n",
        "```\n",
        "\n",
        "*(donde `1160` es el ID asignado a `<|unk|>` en el vocabulario extendido)*\n",
        "\n",
        "---\n",
        "\n",
        "## **Resumen**\n",
        "\n",
        "| Etapa        | Acción                  | Resultado            | Complejidad | Observación                  |   |                    |\n",
        "| ------------ | ----------------------- | -------------------- | ----------- | ---------------------------- | - | ------------------ |\n",
        "| Tokenización | `re.split`              | Palabras y signos    | O(n)        | Separa espacios y puntuación |   |                    |\n",
        "| Limpieza     | `strip` + filtro        | Tokens válidos       | O(n)        | Elimina vacíos               |   |                    |\n",
        "| Mapeo a IDs  | Búsqueda en diccionario | `KeyError` en `'Hi'` | O(n)        | Sin manejo OOV               |   |                    |\n",
        "| Solución     | Usar `<                 | unk                  | >` token    | Codificación robusta         | — | Implementado en V2 |\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión:**\n",
        "El comando `print(tokenizer.encode(text))` con `SimpleTokenizerV1` falla porque el tokenizador no reconoce “Hi” ni “lik”.\n",
        "Este comportamiento es **intencional** en la versión V1 del libro: su propósito es demostrar la necesidad de **ampliar el vocabulario** o **introducir un mecanismo de manejo de palabras desconocidas**, lo que se resolverá en la siguiente versión (`SimpleTokenizerV2`).\n"
      ],
      "metadata": {
        "id": "5puWah1HJOLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.4 Adding special context tokens** (Agregando tokens de contexto especiales)"
      ],
      "metadata": {
        "id": "NMJeK_st20_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op22Ylzd2PvY",
        "outputId": "16f3f897-c389-45ce-fc00-b8c2d0c91227"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Construir la base del **nuevo vocabulario** de tokens, eliminando duplicados y ordenándolos alfabéticamente.\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* `set(preprocessed)`\n",
        "\n",
        "  * Convierte la lista `preprocessed` en un **conjunto** (`set`), eliminando repeticiones.\n",
        "  * Los conjuntos en Python están implementados como **tablas hash**, lo que permite verificar y almacenar unicidad en tiempo promedio **O(1)** por operación.\n",
        "  * Resultado: todos los tokens únicos del corpus.\n",
        "\n",
        "* `list(...)`\n",
        "\n",
        "  * Convierte el conjunto de nuevo en una lista indexable.\n",
        "  * Esto es necesario porque los conjuntos son desordenados y no soportan orden ni indexación.\n",
        "\n",
        "* `sorted(...)`\n",
        "\n",
        "  * Ordena lexicográficamente la lista resultante según el código Unicode de cada carácter.\n",
        "  * El algoritmo usado es **Timsort** (estable, O(n log n)).\n",
        "  * El resultado es reproducible (misma ordenación cada vez), lo cual es crucial para que los IDs sean consistentes en diferentes ejecuciones.\n",
        "\n",
        "**Resultado:**\n",
        "Una lista `all_tokens` con todos los tokens únicos del texto base (`preprocessed`), ordenados de manera determinista.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Agregar dos **tokens especiales** al vocabulario.\n",
        "\n",
        "**Desglose conceptual:**\n",
        "\n",
        "* `.extend(lista)`\n",
        "\n",
        "  * Añade varios elementos al final de la lista existente (modificación in-place).\n",
        "  * Complejidad O(k), donde *k* es el número de elementos añadidos (aquí, 2).\n",
        "\n",
        "**Tokens agregados:**\n",
        "\n",
        "1. **`<|endoftext|>`**\n",
        "\n",
        "   * Marca el **final de una secuencia textual**.\n",
        "   * Se usa durante el entrenamiento y la generación de texto para indicar cuándo detener la predicción.\n",
        "   * Análogo al token `[EOS]` (*End of Sequence*) en otros modelos.\n",
        "\n",
        "2. **`<|unk|>`**\n",
        "\n",
        "   * Representa cualquier **token desconocido** (*unknown token*).\n",
        "   * Es el símbolo que sustituirá palabras no vistas durante la fase de entrenamiento (manejo OOV, *Out-Of-Vocabulary*).\n",
        "   * En la práctica, este token permite que el modelo funcione sobre cualquier texto, incluso si aparecen palabras nuevas.\n",
        "\n",
        "**Resultado:**\n",
        "`all_tokens` contiene ahora:\n",
        "\n",
        "```\n",
        "[... 'yourself', '<|endoftext|>', '<|unk|>']\n",
        "```\n",
        "\n",
        "Si el vocabulario original tenía *N* tokens, ahora tendrá *N + 2*.\n",
        "\n",
        "---\n",
        "\n",
        "### **3)**\n",
        "\n",
        "```python\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Generar un **diccionario de mapeo** entre cada token y un identificador numérico único (índice entero).\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* `enumerate(all_tokens)`\n",
        "\n",
        "  * Devuelve un iterador que produce pares `(índice, token)`, donde el índice empieza en 0.\n",
        "  * Ejemplo:\n",
        "\n",
        "    ```\n",
        "    0: '!', 1: '\"', 2: \"'\", ..., 1127: '<|endoftext|>', 1128: '<|unk|>'\n",
        "    ```\n",
        "\n",
        "* La comprensión de diccionario:\n",
        "\n",
        "  ```python\n",
        "  {token: integer for integer, token in enumerate(all_tokens)}\n",
        "  ```\n",
        "\n",
        "  * Invierte el orden del par, de `(índice, token)` a `(token, índice)`.\n",
        "  * Cada token textual se convierte en **clave**, y su posición en **valor entero**.\n",
        "  * Implementación interna: el diccionario (`dict`) es una tabla hash con búsqueda promedio O(1).\n",
        "\n",
        "**Importancia:**\n",
        "\n",
        "* Este diccionario constituye el **vocabulario formal del modelo**.\n",
        "* El token `<|unk|>` permitirá asignar un valor por defecto a cualquier palabra fuera del vocabulario conocido.\n",
        "* El token `<|endoftext|>` servirá como marcador de final de secuencia en el entrenamiento autoregresivo del LLM.\n",
        "\n",
        "---\n",
        "\n",
        "### **4)**\n",
        "\n",
        "```python\n",
        "print(len(vocab.items()))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Verificar el tamaño total del vocabulario extendido.\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `vocab.items()` devuelve una *vista* (`dict_items`) con todos los pares `(token, id)` del diccionario.\n",
        "* `len(vocab.items())` cuenta cuántos pares existen.\n",
        "* Este valor equivale al **número total de tokens únicos + 2** (por los tokens especiales añadidos).\n",
        "\n",
        "**Ejemplo de salida:**\n",
        "\n",
        "```\n",
        "1130\n",
        "```\n",
        "\n",
        "si el vocabulario original tenía 1128 tokens únicos.\n",
        "\n",
        "**Complejidad temporal:**\n",
        "\n",
        "* `len()` sobre una estructura `dict` o `dict_items` es O(1), ya que el tamaño se almacena internamente en el encabezado del objeto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen funcional**\n",
        "\n",
        "| Línea                       | Acción                   | Resultado              | Complejidad |\n",
        "| --------------------------- | ------------------------ | ---------------------- | ----------- |\n",
        "| `set(preprocessed)`         | Eliminar duplicados      | Tokens únicos          | O(n)        |\n",
        "| `sorted(list(...))`         | Ordenar tokens           | Lista ordenada         | O(n log n)  |\n",
        "| `.extend([...])`            | Añadir tokens especiales | Lista +2 elementos     | O(1)        |\n",
        "| Diccionario por comprensión | Crear mapeo token→ID     | Diccionario `vocab`    | O(n)        |\n",
        "| `len(vocab.items())`        | Contar tokens totales    | Tamaño del vocabulario | O(1)        |\n",
        "\n",
        "---\n",
        "\n",
        "### **Concepto general**\n",
        "\n",
        "Este bloque crea un **vocabulario robusto**, apto para usarse en la versión mejorada del tokenizador (`SimpleTokenizerV2`):\n",
        "\n",
        "* Garantiza consistencia (orden determinista).\n",
        "* Soporta tokens fuera del vocabulario (`<|unk|>`).\n",
        "* Integra delimitadores de secuencia (`<|endoftext|>`).\n",
        "* Es la base para entrenar embeddings en modelos LLM de tipo GPT o Transformer decoder.\n",
        "\n",
        "En síntesis:\n",
        "\n",
        "> Aquí se define el *espacio discreto completo de símbolos* que el modelo será capaz de representar, asegurando que todo texto —conocido o no— pueda codificarse de forma numérica sin errores.\n"
      ],
      "metadata": {
        "id": "ip4JPgzqJ6AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqVxTJj64WLy",
        "outputId": "0030a1ea-0e36-48b1-884e-3b1eef40fa53"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Recorrer e imprimir los **últimos cinco elementos** del diccionario `vocab`, mostrando los tokens y sus identificadores numéricos más altos (generalmente los tokens añadidos al final, como `<|endoftext|>` y `<|unk|>`).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Desglose técnico:**\n",
        "\n",
        "##### a) `vocab.items()`\n",
        "\n",
        "* Devuelve una **vista de elementos** del diccionario (`dict_items`), que contiene todos los pares `(clave, valor)` del diccionario `vocab`.\n",
        "\n",
        "  * Cada elemento es una tupla: `(token, id_entero)`.\n",
        "  * Ejemplo:\n",
        "\n",
        "    ```\n",
        "    ('yourself', 1127), ('<|endoftext|>', 1128), ('<|unk|>', 1129)\n",
        "    ```\n",
        "\n",
        "* En Python 3.7+, los diccionarios **preservan el orden de inserción**, por lo que los últimos elementos son los últimos tokens añadidos a `all_tokens`.\n",
        "\n",
        "##### b) `list(vocab.items())`\n",
        "\n",
        "* Convierte la vista `dict_items` en una **lista indexable**.\n",
        "* Esto permite aplicar slicing (`[-5:]`) para acceder a los últimos cinco elementos.\n",
        "* Complejidad temporal: **O(n)**, ya que se copian todas las entradas del diccionario a una nueva lista.\n",
        "\n",
        "##### c) `[-5:]`\n",
        "\n",
        "* Es un **slice** (rebanado) que selecciona los últimos cinco elementos de la lista.\n",
        "* Ejemplo:\n",
        "\n",
        "  ```python\n",
        "  list(vocab.items())[-5:]\n",
        "  # → [('your', 1125), ('yourself', 1126), ('<|endoftext|>', 1127), ('<|unk|>', 1128)]\n",
        "  ```\n",
        "* Si el diccionario tiene menos de 5 elementos, devuelve todos.\n",
        "\n",
        "##### d) `enumerate(...)`\n",
        "\n",
        "* Añade un contador `i` que empieza en 0 por defecto.\n",
        "\n",
        "* Devuelve pares `(i, item)`, donde:\n",
        "\n",
        "  * `i` es el índice del elemento en el bucle,\n",
        "  * `item` es la tupla `(token, id)`.\n",
        "\n",
        "* Ejemplo:\n",
        "\n",
        "  ```\n",
        "  0 ('your', 1125)\n",
        "  1 ('yourself', 1126)\n",
        "  2 ('<|endoftext|>', 1127)\n",
        "  3 ('<|unk|>', 1128)\n",
        "  ```\n",
        "\n",
        "**Complejidad general del bucle:**\n",
        "\n",
        "* O(n) por la conversión `list(vocab.items())`, pero solo recorre 5 elementos en la iteración.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "print(item)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Imprimir cada tupla `(token, id)` generada por el bucle.\n",
        "\n",
        "* Cada línea de salida mostrará un par token–índice del vocabulario.\n",
        "* Ejemplo de salida real:\n",
        "\n",
        "  ```\n",
        "  ('your', 1125)\n",
        "  ('yourself', 1126)\n",
        "  ('<|endoftext|>', 1127)\n",
        "  ('<|unk|>', 1128)\n",
        "  ```\n",
        "\n",
        "**Detalles técnicos de `print`:**\n",
        "\n",
        "* `print()` convierte cada objeto en su representación de texto (`str()` o `repr()`) y lo envía al flujo estándar de salida (`sys.stdout`).\n",
        "* Cada llamada termina en salto de línea `\\n` por defecto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen funcional**\n",
        "\n",
        "| Etapa            | Acción                    | Resultado              | Complejidad |\n",
        "| ---------------- | ------------------------- | ---------------------- | ----------- |\n",
        "| `vocab.items()`  | Obtener pares (token, id) | vista `dict_items`     | O(1)        |\n",
        "| `list(...)`      | Convertir a lista         | lista indexable        | O(n)        |\n",
        "| `[-5:]`          | Tomar últimos 5           | sublista               | O(1)        |\n",
        "| `enumerate(...)` | Añadir índice             | pares (i, (token, id)) | O(1)        |\n",
        "| `print(item)`    | Mostrar resultado         | salida en consola      | O(1)        |\n",
        "\n",
        "---\n",
        "\n",
        "### **Resultado conceptual**\n",
        "\n",
        "Este fragmento sirve como verificación visual del **final del vocabulario** y confirma que los **tokens especiales** (`<|endoftext|>`, `<|unk|>`) fueron correctamente añadidos y asignados con los índices más altos.\n",
        "\n",
        "En contexto, los últimos valores impresos deben ser similares a:\n",
        "\n",
        "```\n",
        "('your', 1125)\n",
        "('yourself', 1126)\n",
        "('<|endoftext|>', 1127)\n",
        "('<|unk|>', 1128)\n",
        "```\n",
        "\n",
        "Esto confirma que:\n",
        "\n",
        "* El vocabulario está ordenado lexicográficamente.\n",
        "* Los dos tokens especiales fueron añadidos al final de la lista.\n",
        "* Los índices son consecutivos y únicos.\n",
        "\n",
        "En síntesis: este bucle actúa como **validación final de la integridad del vocabulario** antes de proceder a la implementación del `SimpleTokenizerV2`.\n"
      ],
      "metadata": {
        "id": "dYunPImUKfp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Listing 2.4 A simple text tokenizer that handles unknown words**\n",
        "# **Listado 2.4 Un tokenizador de texto simple que maneja palabras desconocidas**"
      ],
      "metadata": {
        "id": "IF4WunNQ13nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Bv4KOzTV7lKj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva del diseño y comportamiento de `SimpleTokenizerV2`, línea por línea y a nivel conceptual:\n",
        "\n",
        "---\n",
        "\n",
        "## **Definición general**\n",
        "\n",
        "```python\n",
        "class SimpleTokenizerV2:\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Implementar una versión **tolerante a palabras desconocidas** (*Out-Of-Vocabulary*, OOV) del tokenizador simple desarrollado previamente (`SimpleTokenizerV1`).\n",
        "\n",
        "**Contexto técnico:**\n",
        "\n",
        "* La versión anterior lanzaba un `KeyError` cuando un token no existía en el vocabulario.\n",
        "* Esta nueva versión introduce un **mecanismo de respaldo** (`<|unk|>`) que permite procesar cualquier entrada textual.\n",
        "* Representa la transición entre un tokenizador experimental y uno funcionalmente robusto para entrenamiento real de modelos.\n",
        "\n",
        "---\n",
        "\n",
        "## **1) Constructor**\n",
        "\n",
        "```python\n",
        "def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "```\n",
        "\n",
        "### **Propósito:**\n",
        "\n",
        "Inicializar los mapeos **token → id** y **id → token** para permitir conversión bidireccional.\n",
        "\n",
        "### **Detalles técnicos:**\n",
        "\n",
        "* `self.str_to_int = vocab`\n",
        "\n",
        "  * Guarda una referencia al diccionario base que mapea cadenas (tokens) a índices enteros.\n",
        "  * Este diccionario contiene ahora también los tokens especiales `<|endoftext|>` y `<|unk|>`.\n",
        "  * Acceso promedio: **O(1)** (tabla hash).\n",
        "\n",
        "* `self.int_to_str = {i: s for s, i in vocab.items()}`\n",
        "\n",
        "  * Inversión de claves y valores para decodificación.\n",
        "  * Construye un nuevo diccionario: cada id entero apunta a su token textual.\n",
        "  * Ejemplo:\n",
        "\n",
        "    ```\n",
        "    {0: '!', 1: '\"', ..., 1127: '<|endoftext|>', 1128: '<|unk|>'}\n",
        "    ```\n",
        "  * Complejidad de construcción: **O(n)** donde *n* es el número de tokens.\n",
        "  * Requiere recorrer el vocabulario completo una vez.\n",
        "\n",
        "**Resultado:**\n",
        "Dos estructuras complementarias que permiten conversión directa entre representación textual y numérica.\n",
        "\n",
        "---\n",
        "\n",
        "## **2) Método `encode()`**\n",
        "\n",
        "```python\n",
        "def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "```\n",
        "\n",
        "### **Propósito:**\n",
        "\n",
        "Convertir texto crudo (`str`) en una secuencia de identificadores numéricos (`List[int]`), asignando `<|unk|>` a los tokens no presentes en el vocabulario.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 1 — Tokenización base**\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "```\n",
        "\n",
        "* Usa la misma expresión regular que en la versión V1 para segmentar texto:\n",
        "\n",
        "  * `(...)` grupo de captura → conserva los separadores.\n",
        "  * `[,.?_!\"()']` → coincide con signos de puntuación básicos.\n",
        "  * `|--` → detecta doble guion literal.\n",
        "  * `|\\s` → divide por cualquier espacio en blanco.\n",
        "* Genera una lista con palabras, signos y separadores vacíos.\n",
        "* Complejidad: **O(n)** sobre el número de caracteres.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Limpieza de tokens**\n",
        "\n",
        "```python\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "```\n",
        "\n",
        "* `item.strip()` elimina espacios al inicio y al final del token.\n",
        "\n",
        "* El `if` descarta cadenas vacías resultantes.\n",
        "\n",
        "* Resultado: una lista limpia con palabras y signos.\n",
        "\n",
        "* Ejemplo:\n",
        "\n",
        "  ```\n",
        "  ['Hi', ',', 'do', 'you', 'lik', 'tea', '?']\n",
        "  ```\n",
        "\n",
        "* Complejidad: **O(m)**, donde *m* = número de tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Manejo de palabras desconocidas**\n",
        "\n",
        "```python\n",
        "preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "```\n",
        "\n",
        "* Verifica para cada token si existe en el vocabulario (`self.str_to_int`):\n",
        "\n",
        "  * Si **existe**, lo mantiene igual.\n",
        "  * Si **no existe**, lo reemplaza por el token especial `<|unk|>`.\n",
        "* Esto evita excepciones `KeyError` y asegura que todos los tokens puedan convertirse en IDs.\n",
        "* Implementación:\n",
        "\n",
        "  * Búsqueda hash promedio O(1) por token.\n",
        "  * Complejidad total de esta etapa: O(m).\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "```python\n",
        "['Hi', ',', 'do', 'you', 'lik', 'tea', '?']\n",
        "↓\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?']\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Conversión a IDs**\n",
        "\n",
        "```python\n",
        "ids = [self.str_to_int[s] for s in preprocessed]\n",
        "```\n",
        "\n",
        "* Sustituye cada token textual por su identificador numérico.\n",
        "\n",
        "* Como todos los tokens ya están garantizados en el vocabulario, no hay errores.\n",
        "\n",
        "* Ejemplo hipotético:\n",
        "\n",
        "  ```\n",
        "  {'<|unk|>': 1128, ',': 3, 'do': 42, 'you': 57, 'tea': 89, '?': 9}\n",
        "  ↓\n",
        "  [1128, 3, 42, 57, 1128, 89, 9]\n",
        "  ```\n",
        "\n",
        "* Complejidad temporal: **O(m)**.\n",
        "\n",
        "* Complejidad espacial: O(m) para la lista de salida.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 5 — Retorno**\n",
        "\n",
        "```python\n",
        "return ids\n",
        "```\n",
        "\n",
        "* Devuelve una lista de enteros (`List[int]`).\n",
        "* Esta lista es adecuada para ser convertida en tensor de entrada en frameworks de entrenamiento (PyTorch, TensorFlow).\n",
        "\n",
        "---\n",
        "\n",
        "## **3) Método `decode()`**\n",
        "\n",
        "```python\n",
        "def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "### **Propósito:**\n",
        "\n",
        "Convertir una lista de identificadores numéricos en texto legible, aplicando corrección de espacios.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 1 — Reconstrucción de tokens**\n",
        "\n",
        "```python\n",
        "[self.int_to_str[i] for i in ids]\n",
        "```\n",
        "\n",
        "* Usa el diccionario inverso `int_to_str` para traducir cada ID a su token textual.\n",
        "* Complejidad: **O(m)**, acceso hash promedio O(1) por elemento.\n",
        "* Resultado intermedio (lista de strings).\n",
        "  Ejemplo:\n",
        "\n",
        "  ```\n",
        "  ['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Unión con espacios**\n",
        "\n",
        "```python\n",
        "text = \" \".join([...])\n",
        "```\n",
        "\n",
        "* Concatena los tokens con un espacio como separador.\n",
        "\n",
        "* Ejemplo:\n",
        "\n",
        "  ```\n",
        "  '<|unk|> , do you <|unk|> tea ?'\n",
        "  ```\n",
        "\n",
        "* Complejidad: O(m) respecto a longitud del texto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Limpieza tipográfica**\n",
        "\n",
        "```python\n",
        "text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "```\n",
        "\n",
        "* Usa una expresión regular para eliminar espacios redundantes antes de signos de puntuación.\n",
        "* Patrón:\n",
        "\n",
        "  * `\\s+` → uno o más espacios.\n",
        "  * `([,.?!\"()'])` → grupo de signos de puntuación comunes.\n",
        "* Reemplazo: `r'\\1'` → mantiene solo el signo, eliminando el espacio previo.\n",
        "* Ejemplo:\n",
        "\n",
        "  ```\n",
        "  '<|unk|> , do you <|unk|> tea ?'\n",
        "  ↓\n",
        "  '<|unk|>, do you <|unk|> tea?'\n",
        "  ```\n",
        "* Complejidad: O(n) sobre longitud de la cadena.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Retorno**\n",
        "\n",
        "```python\n",
        "return text\n",
        "```\n",
        "\n",
        "* Devuelve la cadena reconstruida.\n",
        "* El resultado mantiene coherencia gramatical básica, aunque puede incluir tokens `<|unk|>` que representan términos desconocidos.\n",
        "\n",
        "**Salida final:**\n",
        "\n",
        "```\n",
        "'<|unk|>, do you <|unk|> tea?'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **4) Análisis de robustez**\n",
        "\n",
        "| Característica      | `V1`         | `V2`       | Efecto                    |        |           |     |                    |\n",
        "| ------------------- | ------------ | ---------- | ------------------------- | ------ | --------- | --- | ------------------ |\n",
        "| Manejo de OOV       | ❌            | ✅          | Evita `KeyError`          |        |           |     |                    |\n",
        "| Tokens especiales   | ❌            | ✅ (`<      | unk                       | >`, `< | endoftext | >`) | Cobertura completa |\n",
        "| Consistencia de IDs | ✅            | ✅          | Índices fijos             |        |           |     |                    |\n",
        "| Formato de salida   | Básico       | Limpio     | Espacios corregidos       |        |           |     |                    |\n",
        "| Uso en modelos      | Experimental | Entrenable | Compatible con embeddings |        |           |     |                    |\n",
        "\n",
        "---\n",
        "\n",
        "## **5) Complejidad global**\n",
        "\n",
        "| Operación               | Complejidad temporal | Complejidad espacial |\n",
        "| ----------------------- | -------------------- | -------------------- |\n",
        "| Tokenización y limpieza | O(n)                 | O(n)                 |\n",
        "| Verificación OOV        | O(n)                 | O(n)                 |\n",
        "| Mapeo a IDs             | O(n)                 | O(n)                 |\n",
        "| Decodificación          | O(n)                 | O(n)                 |\n",
        "\n",
        "Donde *n* es el número de tokens procesados.\n",
        "\n",
        "---\n",
        "\n",
        "## **6) Importancia dentro del pipeline del LLM**\n",
        "\n",
        "El `SimpleTokenizerV2` introduce el primer **componente de tolerancia semántica** en el preprocesamiento:\n",
        "\n",
        "* Permite generalizar la codificación a entradas arbitrarias.\n",
        "* Garantiza que el modelo pueda recibir texto sin fallar.\n",
        "* Establece la base para el **vocabulario cerrado** usado en la pre-entrenamiento y fine-tuning de modelos autoregresivos tipo GPT.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión:**\n",
        "`SimpleTokenizerV2` convierte el prototipo V1 en un tokenizador **seguro y universal**, capaz de procesar cualquier texto de entrada sin excepciones, preservando el mapeo bidireccional entre texto e índices y respetando la estructura léxica requerida por los modelos de lenguaje basados en transformadores.\n"
      ],
      "metadata": {
        "id": "8DgybLEoK9-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = 'Hello, do you like tea?'\n",
        "text2 = 'In the sunlit terraces of the palace.'\n",
        "text = ' <|endoftext|> '.join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehFf5s8NDjPv",
        "outputId": "67aaab5e-d193-41ab-ca8e-50b6b1ed4290"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, con análisis estructural, sintáctico y funcional:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "text1 = 'Hello, do you like tea?'\n",
        "text2 = 'In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Definir dos fragmentos de texto (`str`) que simulan **entradas independientes** dentro de un corpus o dataset de entrenamiento.\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* Ambos objetos son instancias del tipo `str` de Python, almacenadas como secuencias inmutables de *code points* Unicode.\n",
        "* En memoria, Python usa una representación compacta (internamente UTF-8 o UCS-2/UCS-4 según compilación), y ambas cadenas residen en el heap con sus referencias gestionadas por el recolector de basura.\n",
        "* El texto está formado por palabras, espacios y signos de puntuación (`','`, `'?'`, `'.'`), los cuales más adelante serán segmentados por el tokenizador.\n",
        "\n",
        "**Semántica en el flujo del modelo:**\n",
        "\n",
        "* Cada `textX` representa una unidad de entrada independiente (por ejemplo, un documento, diálogo o frase).\n",
        "* En entrenamiento autoregresivo, se suelen concatenar múltiples secuencias con un **token delimitador** para permitir al modelo aprender los límites de contexto.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "text = ' <|endoftext|> '.join((text1, text2))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Concatenar las dos cadenas `text1` y `text2`, insertando entre ellas el **token especial de final de texto** `<|endoftext|>`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Desglose técnico:**\n",
        "\n",
        "##### a) `' <|endoftext|> '`\n",
        "\n",
        "* Es una cadena literal que contiene:\n",
        "\n",
        "  * un espacio inicial `' '`\n",
        "  * el token especial `<|endoftext|>`\n",
        "  * un espacio final `' '`\n",
        "* Este token actúa como **delimitador semántico** entre fragmentos, permitiendo distinguir el final de una secuencia y el inicio de otra dentro de un mismo batch textual.\n",
        "* Este token fue añadido explícitamente al vocabulario en pasos anteriores.\n",
        "\n",
        "##### b) `.join((text1, text2))`\n",
        "\n",
        "* `.join(iterable)` concatena los elementos del iterable especificado, separándolos por la cadena sobre la que se invoca el método.\n",
        "* El iterable aquí es una **tupla**: `(text1, text2)`.\n",
        "\n",
        "  * Las tuplas son inmutables y de acceso indexado O(1).\n",
        "* `join` recorre los elementos, los concatena, y **devuelve una nueva cadena**.\n",
        "\n",
        "  * Implementación interna: crea un buffer de tamaño exacto mediante cálculo previo de longitud total → complejidad **O(n)** sobre la suma de longitudes.\n",
        "\n",
        "##### c) Resultado exacto:\n",
        "\n",
        "```\n",
        "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "**Estructura conceptual resultante:**\n",
        "\n",
        "```\n",
        "[secuencia_1] + [token delimitador] + [secuencia_2]\n",
        "```\n",
        "\n",
        "Este patrón es típico en *datasets concatenados de texto plano*, donde el token `<|endoftext|>` reemplaza saltos de documento o marcadores de fin de muestra.\n",
        "\n",
        "---\n",
        "\n",
        "### **3)**\n",
        "\n",
        "```python\n",
        "print(text)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Mostrar en consola el resultado de la concatenación, verificando la correcta inserción del delimitador especial.\n",
        "\n",
        "**Salida esperada:**\n",
        "\n",
        "```\n",
        "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
        "```\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `print()` llama internamente a `sys.stdout.write()` con una conversión implícita `str()` sobre su argumento.\n",
        "* Añade un salto de línea final (`\\n`) por defecto.\n",
        "* Operación O(n) en la longitud total del texto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Análisis conceptual**\n",
        "\n",
        "| Elemento         | Función en la arquitectura del LLM        | Implicación                                                          |                                           |                                            |\n",
        "| ---------------- | ----------------------------------------- | -------------------------------------------------------------------- | ----------------------------------------- | ------------------------------------------ |\n",
        "| `text1`, `text2` | Fragmentos independientes del corpus      | Simulan secuencias separadas de entrenamiento                        |                                           |                                            |\n",
        "| `<               | endoftext                                 | >`                                                                   | Token delimitador aprendido por el modelo | Enseña al modelo dónde termina una muestra |\n",
        "| `.join()`        | Concatenación controlada                  | Permite construir datasets textuales continuos                       |                                           |                                            |\n",
        "| Resultado final  | Texto continuo con marcador de separación | Base para el entrenamiento autoregresivo sin pérdida de segmentación |                                           |                                            |\n",
        "\n",
        "---\n",
        "\n",
        "### **Complejidad total**\n",
        "\n",
        "| Operación             | Complejidad temporal             | Espacial        |\n",
        "| --------------------- | -------------------------------- | --------------- |\n",
        "| Creación de literales | O(1)                             | O(1)            |\n",
        "| `.join()`             | O(n₁ + n₂ + k)` (longitud total) | O(n₁ + n₂ + k)` |\n",
        "| `print()`             | O(n₁ + n₂ + k)`                  | O(1)            |\n",
        "\n",
        "donde *n₁* y *n₂* son las longitudes de `text1` y `text2`, y *k* la longitud del delimitador `' <|endoftext|> '`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Significado dentro del pipeline de un LLM**\n",
        "\n",
        "Este paso demuestra la **integración práctica de tokens especiales** dentro del flujo textual.\n",
        "En los modelos tipo GPT:\n",
        "\n",
        "* `<|endoftext|>` marca la **frontera de contexto** en el corpus concatenado.\n",
        "* Durante la inferencia, el modelo puede generar este token para indicar que debe **detener la producción de texto**.\n",
        "* Durante el entrenamiento, el modelo aprende que la probabilidad de este token aumenta cuando una muestra llega a su fin.\n",
        "\n",
        "En resumen:\n",
        "\n",
        "> Este fragmento construye una cadena compuesta que respeta la semántica de fin de texto, asegurando continuidad sintáctica y segmentación explícita entre unidades lingüísticas —un paso esencial en la preparación de corpora para el entrenamiento autoregresivo de un LLM.\n"
      ],
      "metadata": {
        "id": "ocWRl6eTRygT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7NSOTWGENal",
        "outputId": "d0e2082f-e073-4fac-d1ba-fd34ff5e8a0f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva del comportamiento de este bloque, tanto en nivel de ejecución como en su papel dentro del flujo del modelo de lenguaje:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Crear una instancia del tokenizador robusto (`SimpleTokenizerV2`) empleando el vocabulario extendido que incluye los tokens especiales `<|endoftext|>` y `<|unk|>`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Comportamiento interno:**\n",
        "\n",
        "* Se ejecuta el **constructor** `__init__()` definido en la clase:\n",
        "\n",
        "  ```python\n",
        "  def __init__(self, vocab):\n",
        "      self.str_to_int = vocab\n",
        "      self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "  ```\n",
        "\n",
        "* **Estructuras creadas:**\n",
        "\n",
        "  1. `self.str_to_int`: diccionario que mapea **token → ID entero**\n",
        "\n",
        "     * Ejemplo:\n",
        "\n",
        "       ```\n",
        "       {',': 3, '?': 9, 'Hello': 240, '<|endoftext|>': 1127, '<|unk|>': 1128}\n",
        "       ```\n",
        "  2. `self.int_to_str`: diccionario inverso **ID → token**, generado mediante comprensión:\n",
        "\n",
        "     ```python\n",
        "     {0: '!', 1: '\"', ..., 1127: '<|endoftext|>', 1128: '<|unk|>'}\n",
        "     ```\n",
        "\n",
        "* **Complejidad:**\n",
        "\n",
        "  * Construcción: O(n) (recorre el vocabulario completo).\n",
        "  * Accesos posteriores: O(1) promedio (hash table).\n",
        "\n",
        "* **Resultado:**\n",
        "  Un tokenizador funcional capaz de codificar y decodificar cualquier texto sin errores de clave.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "print(tokenizer.encode(text))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Aplicar el proceso de **tokenización y codificación numérica** sobre la cadena `text` que contiene el delimitador `<|endoftext|>` y mostrar el resultado.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Flujo interno dentro de `encode()`**\n",
        "\n",
        "```python\n",
        "def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 1 — Tokenización regular**\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "```\n",
        "\n",
        "**Entrada:**\n",
        "\n",
        "```\n",
        "text = 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "**Patrón:**\n",
        "\n",
        "* `[,.?_!\"()']` → divide por puntuación básica.\n",
        "* `|--` → captura doble guion literal.\n",
        "* `|\\s` → divide por espacios.\n",
        "\n",
        "**Resultado inicial (lista con separadores y vacíos):**\n",
        "\n",
        "```\n",
        "['Hello', ',', '', ' ', 'do', ' ', 'you', ' ', 'like', ' ', 'tea', '?', ' ', '<|endoftext|>', ' ', 'In', ' ', 'the', ' ', 'sunlit', ' ', 'terraces', ' ', 'of', ' ', 'the', ' ', 'palace', '.', '']\n",
        "```\n",
        "\n",
        "**Complejidad:** O(n) sobre número de caracteres.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Limpieza**\n",
        "\n",
        "```python\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "```\n",
        "\n",
        "* `strip()` elimina espacios al inicio y fin.\n",
        "* `if item.strip()` descarta vacíos.\n",
        "* Resultado limpio:\n",
        "\n",
        "  ```\n",
        "  ['Hello', ',', 'do', 'you', 'like', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "  ```\n",
        "\n",
        "**Complejidad:** O(m), donde *m* = número de tokens detectados.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Sustitución de palabras desconocidas**\n",
        "\n",
        "```python\n",
        "preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "```\n",
        "\n",
        "**Proceso:**\n",
        "\n",
        "* Comprueba si cada token está en `self.str_to_int`.\n",
        "* Si no, lo reemplaza por `<|unk|>`.\n",
        "\n",
        "**Caso particular:**\n",
        "\n",
        "* `\"Hello\"` y `\"like\"` probablemente **no existen** en el vocabulario del cuento *The Verdict*.\n",
        "* Por tanto, serán reemplazados por `<|unk|>`.\n",
        "\n",
        "**Resultado tras esta etapa:**\n",
        "\n",
        "```\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "```\n",
        "\n",
        "**Importante:**\n",
        "El token `<|endoftext|>` **sí existe** en el vocabulario, por lo que se conserva literalmente.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Mapeo token → ID**\n",
        "\n",
        "```python\n",
        "ids = [self.str_to_int[s] for s in preprocessed]\n",
        "```\n",
        "\n",
        "* Convierte cada token textual en su identificador numérico.\n",
        "* Acceso a diccionario: O(1) promedio por token.\n",
        "* Complejidad total: O(m).\n",
        "\n",
        "**Ejemplo de salida hipotética:**\n",
        "\n",
        "```\n",
        "[1128, 3, 42, 57, 1128, 89, 9, 1127, 215, 27, 390, 420, 84, 27, 500, 7]\n",
        "```\n",
        "\n",
        "donde:\n",
        "\n",
        "| Token    | ID (ejemplo) |    |      |\n",
        "| -------- | ------------ | -- | ---- |\n",
        "| `<       | unk          | >` | 1128 |\n",
        "| `,`      | 3            |    |      |\n",
        "| `do`     | 42           |    |      |\n",
        "| `you`    | 57           |    |      |\n",
        "| `tea`    | 89           |    |      |\n",
        "| `?`      | 9            |    |      |\n",
        "| `<       | endoftext    | >` | 1127 |\n",
        "| `In`     | 215          |    |      |\n",
        "| `the`    | 27           |    |      |\n",
        "| `palace` | 500          |    |      |\n",
        "| `.`      | 7            |    |      |\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 5 — Retorno y visualización**\n",
        "\n",
        "```python\n",
        "return ids\n",
        "```\n",
        "\n",
        "* Devuelve la lista de IDs enteros.\n",
        "* `print()` muestra el resultado en consola como lista Python estándar.\n",
        "\n",
        "**Salida visual esperada:**\n",
        "\n",
        "```\n",
        "[1128, 3, 42, 57, 1128, 89, 9, 1127, 215, 27, 390, 420, 84, 27, 500, 7]\n",
        "```\n",
        "\n",
        "*(Los valores dependen del orden real del vocabulario.)*\n",
        "\n",
        "---\n",
        "\n",
        "## **Análisis conceptual**\n",
        "\n",
        "| Elemento   | Significado funcional        | Papel en el LLM                               |                                                          |                                               |\n",
        "| ---------- | ---------------------------- | --------------------------------------------- | -------------------------------------------------------- | --------------------------------------------- |\n",
        "| `<         | unk                          | >`                                            | Token de sustitución para palabras fuera del vocabulario | Permite codificar cualquier texto sin errores |\n",
        "| `<         | endoftext                    | >`                                            | Delimitador explícito entre secuencias                   | Enseña al modelo los límites contextuales     |\n",
        "| `encode()` | Conversión texto → índices   | Base para construir tensores de entrenamiento |                                                          |                                               |\n",
        "| `vocab`    | Espacio discreto de símbolos | Define el universo léxico del modelo          |                                                          |                                               |\n",
        "\n",
        "---\n",
        "\n",
        "### **Complejidad global del método**\n",
        "\n",
        "| Etapa                | Operación        | Complejidad temporal | Espacial |\n",
        "| -------------------- | ---------------- | -------------------- | -------- |\n",
        "| Tokenización (regex) | `re.split()`     | O(n)                 | O(n)     |\n",
        "| Limpieza             | `strip` + filtro | O(n)                 | O(n)     |\n",
        "| Sustitución OOV      | Búsqueda hash    | O(n)                 | O(n)     |\n",
        "| Mapeo a IDs          | Diccionario hash | O(n)                 | O(n)     |\n",
        "\n",
        "Donde *n* es el número de caracteres o tokens procesados.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretación dentro del pipeline del LLM**\n",
        "\n",
        "Este paso completa la fase de **codificación simbólica robusta**:\n",
        "\n",
        "1. Garantiza que cualquier texto pueda transformarse en una secuencia de enteros, eliminando errores en la fase de batch tokenization.\n",
        "2. Mantiene los límites entre documentos con `<|endoftext|>`.\n",
        "3. Proporciona una representación **determinista y reproducible**, indispensable para entrenar embeddings o modelos de atención.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión:**\n",
        "La instrucción\n",
        "\n",
        "```python\n",
        "print(tokenizer.encode(text))\n",
        "```\n",
        "\n",
        "produce una secuencia de IDs enteros que representa de forma numérica la cadena\n",
        "\n",
        "```\n",
        "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
        "```\n",
        "\n",
        "con manejo seguro de tokens desconocidos mediante `<|unk|>` y preservación del token `<|endoftext|>`.\n",
        "Este es el paso en que el texto humano se convierte, por primera vez, en una estructura formal apta para el aprendizaje en un modelo de lenguaje.\n"
      ],
      "metadata": {
        "id": "OAA9CK7WSjwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c-f-9ogGwb0",
        "outputId": "78cf8f78-463f-4257-d89e-a6c9538fc6da"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, incluyendo funcionamiento interno completo, estructura de datos implicadas y relevancia dentro del flujo de un LLM:\n",
        "\n",
        "---\n",
        "\n",
        "## **1)**\n",
        "\n",
        "```python\n",
        "print(tokenizer.decode(tokenizer.encode(text)))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Ejecutar el ciclo completo **texto → tokens → IDs → texto**, validando que el tokenizador `SimpleTokenizerV2` sea **reversible**, consistente y tolerante a palabras desconocidas (*out-of-vocabulary*, OOV).\n",
        "\n",
        "---\n",
        "\n",
        "## **2) Flujo general de ejecución**\n",
        "\n",
        "La expresión se evalúa de adentro hacia afuera:\n",
        "\n",
        "1. `tokenizer.encode(text)`\n",
        "   → convierte el texto crudo en una lista de identificadores enteros.\n",
        "2. `tokenizer.decode(...)`\n",
        "   → traduce esos identificadores de vuelta a texto.\n",
        "3. `print(...)`\n",
        "   → muestra la cadena reconstruida resultante.\n",
        "\n",
        "---\n",
        "\n",
        "## **3) Entrada inicial**\n",
        "\n",
        "```python\n",
        "text = 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "El texto contiene:\n",
        "\n",
        "* palabras comunes (`do`, `you`, `tea`, `the`, etc.),\n",
        "* palabras potencialmente desconocidas (`Hello`, `like`),\n",
        "* un token especial `<|endoftext|>` que **sí está en el vocabulario**,\n",
        "* puntuación (`?`, `,`, `.`).\n",
        "\n",
        "---\n",
        "\n",
        "## **4) Etapas internas de `encode()`**\n",
        "\n",
        "El método `encode()` ejecuta:\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "ids = [self.str_to_int[s] for s in preprocessed]\n",
        "return ids\n",
        "```\n",
        "\n",
        "### **Etapa 4.1 — Tokenización**\n",
        "\n",
        "Divide el texto en palabras, signos y espacios, conservando separadores.\n",
        "\n",
        "**Salida intermedia:**\n",
        "\n",
        "```\n",
        "['Hello', ',', 'do', 'you', 'like', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "```\n",
        "\n",
        "### **Etapa 4.2 — Sustitución de palabras fuera del vocabulario**\n",
        "\n",
        "* `\"Hello\"` y `\"like\"` no están en el vocabulario del cuento *The Verdict*.\n",
        "* Se reemplazan por `<|unk|>` (token desconocido).\n",
        "\n",
        "**Resultado limpio:**\n",
        "\n",
        "```\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "```\n",
        "\n",
        "### **Etapa 4.3 — Conversión a IDs**\n",
        "\n",
        "Cada token se transforma en su índice entero del vocabulario (`self.str_to_int`).\n",
        "\n",
        "**Ejemplo conceptual:**\n",
        "\n",
        "```\n",
        "[1128, 3, 42, 57, 1128, 89, 9, 1127, 215, 27, 390, 420, 84, 27, 500, 7]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **5) Etapas internas de `decode()`**\n",
        "\n",
        "El método `decode()`:\n",
        "\n",
        "```python\n",
        "def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "### **Etapa 5.1 — Reconstrucción desde IDs**\n",
        "\n",
        "Convierte cada entero a su token textual original con `self.int_to_str[i]`.\n",
        "\n",
        "**Salida intermedia (lista de tokens):**\n",
        "\n",
        "```\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "```\n",
        "\n",
        "### **Etapa 5.2 — Unión con espacios**\n",
        "\n",
        "```python\n",
        "\" \".join([...])\n",
        "```\n",
        "\n",
        "Concatena tokens con un espacio entre ellos:\n",
        "\n",
        "```\n",
        "'<|unk|> , do you <|unk|> tea ? <|endoftext|> In the sunlit terraces of the palace .'\n",
        "```\n",
        "\n",
        "### **Etapa 5.3 — Limpieza tipográfica**\n",
        "\n",
        "```python\n",
        "re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "```\n",
        "\n",
        "* Busca cualquier espacio antes de signos de puntuación y los elimina.\n",
        "* No altera los tokens `<|unk|>` ni `<|endoftext|>`, porque no coinciden con el patrón de signos.\n",
        "\n",
        "**Resultado limpio:**\n",
        "\n",
        "```\n",
        "'<|unk|>, do you <|unk|> tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **6) Etapa final — Impresión**\n",
        "\n",
        "```python\n",
        "print(...)\n",
        "```\n",
        "\n",
        "Muestra la cadena final resultante:\n",
        "\n",
        "```\n",
        "<|unk|>, do you <|unk|> tea? <|endoftext|> In the sunlit terraces of the palace.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **7) Análisis conceptual del resultado**\n",
        "\n",
        "| Elemento    | Significado                                                 | Función en el flujo                    |                                                                    |                                               |\n",
        "| ----------- | ----------------------------------------------------------- | -------------------------------------- | ------------------------------------------------------------------ | --------------------------------------------- |\n",
        "| `<          | unk                                                         | >`                                     | Representa tokens no presentes en el vocabulario (“Hello”, “like”) | Permite codificar sin error cualquier entrada |\n",
        "| `,` `?` `.` | Signos de puntuación reconstruidos sin espacios incorrectos | Verifica consistencia del reensamblado |                                                                    |                                               |\n",
        "| `<          | endoftext                                                   | >`                                     | Delimitador explícito entre fragmentos textuales                   | Enseña límites de contexto al modelo          |\n",
        "\n",
        "---\n",
        "\n",
        "## **8) Propiedades técnicas del proceso completo**\n",
        "\n",
        "| Propiedad                           | Descripción                                                                        | Consecuencia                               |    |                                             |\n",
        "| ----------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------ | -- | ------------------------------------------- |\n",
        "| **Reversibilidad parcial**          | Las palabras conocidas se reconstruyen idénticas; las desconocidas vuelven como `< | unk                                        | >` | Pérdida controlada de información semántica |\n",
        "| **Determinismo**                    | La codificación y decodificación son funciones puras, sin aleatoriedad             | Resultados reproducibles                   |    |                                             |\n",
        "| **Consistencia del espacio léxico** | Solo aparecen tokens del vocabulario                                               | Compatible con entrenamiento de embeddings |    |                                             |\n",
        "| **Complejidad total**               | O(n) en número de tokens, tanto en encode como en decode                           | Escalable para corpus grandes              |    |                                             |\n",
        "\n",
        "---\n",
        "\n",
        "## **9) Significado dentro del pipeline del LLM**\n",
        "\n",
        "Este paso valida la **coherencia bidireccional del tokenizador**, condición necesaria para:\n",
        "\n",
        "* entrenar embeddings consistentes (`token_id → vector → token_id`);\n",
        "* garantizar que cada ID corresponde a un símbolo textual válido;\n",
        "* controlar el flujo de fin de secuencia mediante `<|endoftext|>`;\n",
        "* permitir inferencia segura con textos arbitrarios (manejo OOV).\n",
        "\n",
        "En entrenamiento real, este mecanismo garantiza que:\n",
        "\n",
        "> Cualquier texto del dataset puede ser mapeado a secuencias numéricas válidas y luego reconstruido de manera legible, asegurando estabilidad en la codificación semántica del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## **10) Resultado final**\n",
        "\n",
        "**Salida esperada en consola:**\n",
        "\n",
        "```\n",
        "<|unk|>, do you <|unk|> tea? <|endoftext|> In the sunlit terraces of the palace.\n",
        "```\n",
        "\n",
        "**Interpretación:**\n",
        "\n",
        "* El tokenizador reemplazó “Hello” y “like” por `<|unk|>`.\n",
        "* Preservó puntuación, estructura y delimitador `<|endoftext|>`.\n",
        "* El ciclo `encode → decode` demuestra que el tokenizador es **robusto, determinista y seguro frente a OOV**, cumpliendo con los requisitos de un preprocesador de texto para un LLM autoregresivo.\n"
      ],
      "metadata": {
        "id": "CLHOGePFUKiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 **Byte pair encoding**\n",
        "# 2.5 **Codificación de pares de bytes**"
      ],
      "metadata": {
        "id": "xcieCr6jSH1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nufc0-hcMTwG",
        "outputId": "fd8c804e-5582-4a1e-83e8-0481bcf5f06e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print('tiktoken version: ', version('tiktoken'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rOCBajmMbvf",
        "outputId": "50279f81-2fda-4011-fe18-538c9ba0d3bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version:  0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')\n"
      ],
      "metadata": {
        "id": "9I1U57v-NE2M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunknowPlace.'\n",
        "integers = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "print(integers)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIOVDXUAM0K9",
        "outputId": "11e8553e-a40a-4c0f-a944-4ec4beb1513e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 617, 2954, 2197, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXTBgFHYOvS5",
        "outputId": "c557ff0c-3283-41e2-fb46-f067d22c9309"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunknowPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_akw = 'Akwirw ier'\n",
        "integers_akw = tokenizer.encode(text_akw)\n",
        "print(integers_akw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfd0NymjO7mU",
        "outputId": "711e3599-eeaa-4f0f-92c0-ed48c86a714f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio recomendado del libro:\n",
        "Usa el tokenizador BPE de la biblioteca tiktoken sobre las palabras desconocidas «Akwirw ier» e imprime los IDs de los tokens individuales. Luego, llama a la función decode para cada uno de los enteros resultantes en esta lista para reproducir el mapeo mostrado en la Figura 2.11. Por último, llama al método decode con la lista de IDs de los tokens para verificar si puede reconstruir la entrada original, «Akwirw ier»."
      ],
      "metadata": {
        "id": "2x1QjOpGRvY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in integers_akw:\n",
        "  token_text = tokenizer.decode([token_id])\n",
        "  print(token_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0aE6HA1Q_pK",
        "outputId": "a2be805d-4776-4f1b-85b2-d1f9cdcd79f1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ak\n",
            "w\n",
            "ir\n",
            "w\n",
            " \n",
            "ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings_akw = tokenizer.decode(integers_akw)\n",
        "print(strings_akw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USMLuvQGQaoJ",
        "outputId": "81bbfccd-0b20-4c85-c739-3e8d3fdcac52"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akwirw ier\n"
          ]
        }
      ]
    }
  ]
}